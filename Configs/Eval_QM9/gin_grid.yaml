model:  ["GIN"]
batch_size: [128, 256]
emb_dim: [64, 256]
drop_out: [0, 0.1]
num_layers: [1, 2, 3, 4, 5]
num_mlp_layers: [2]
pooling: ["mean", "sum"]
lr: [0.001]
freeze_gnn: [0]
epochs: [20, 40]
lr_schedule_patience: [5]
lr_scheduler: ["ReduceLROnPlateau"]
lr_scheduler_decay_rate: [0.5]
drop_feat: [0]
min_lr: [2e-5]




