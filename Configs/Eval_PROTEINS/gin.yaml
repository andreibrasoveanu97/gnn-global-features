model:  ["GIN_TU"]
batch_size: [128]
emb_dim: [32]
drop_out: [0.4]
num_layers: [3]
num_mlp_layers: [2]
pooling: ["sum"]
lr: [0.001]
lr2: [0.0005]
freeze_gnn: [0]
epochs: [350]
lr_schedule_patience: [101]
drop_feat: [1]