model:  ["GIN"]
batch_size: [32]
emb_dim: [256]
drop_out: [0.5]
num_layers: [5]
pooling: ["sum"]
lr: [0.001]
lr2: [0.001]
freeze_gnn: [0]
epochs: [1]
lr_schedule_patience: [101]